{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#***Decision Tree***"
      ],
      "metadata": {
        "id": "-yY1b4_QNSoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "A Decision Tree is a supervised machine learning model that represents decisions and their possible consequences as a tree-like structure. It splits data recursively based on features to form branches, with leaves representing outcomes (e.g., class labels in classification). In classification, it works by selecting the best feature at each node to split the data, maximizing separation between classes (e.g., using impurity measures like Gini). The tree is traversed from root to leaf to predict the class of new data."
      ],
      "metadata": {
        "id": "2RkzDoxvOh5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "Answer:-\n",
        "- **Gini Impurity**: Measures the probability of incorrectly classifying a randomly chosen element if labeled randomly according to the class distribution. Formula: Gini(p) = 1 - Σ(p_i^2), where p_i is the probability of class i. Lower Gini indicates purer nodes.\n",
        "\n",
        "\n",
        "- **Entropy**: Measures uncertainty or disorder in the data. Formula: Entropy(p) = - Σ(p_i * log2(p_i)). Zero entropy means pure node.\n",
        "They impact splits by guiding feature selection: the feature that minimizes Gini or Entropy (maximizes information gain) is chosen, leading to more homogeneous child nodes. Gini is faster; Entropy may give slightly better splits in some cases."
      ],
      "metadata": {
        "id": "jIJiXEKuOh04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "Answer:-\n",
        "- **Pre-Pruning**: Stops tree growth early during training (e.g., limiting max_depth or min_samples_split).\n",
        "- **Post-Pruning**: Grows the full tree, then removes branches (e.g., using cost complexity pruning).\n",
        "Difference: Pre-Pruning is proactive (prevents overfitting during build); Post-Pruning is reactive (trims after build).\n",
        "Advantage of Pre-Pruning: Faster training (e.g., for large dataset with 45 features).\n",
        "Advantage of Post-Pruning: Potentially higher accuracy by pruning only unnecessary branches."
      ],
      "metadata": {
        "id": "gb4PaX2cOhyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Information Gain (IG) is the reduction in entropy or impurity after splitting on a feature. Formula: IG = Entropy(parent) - Σ(weighted Entropy(child)). It's important for choosing the best split as it quantifies how much uncertainty is reduced, ensuring the tree is efficient and interpretable (e.g., splitting on age=77 in dataset maximizes IG for classification)."
      ],
      "metadata": {
        "id": "Tr3YNXDAOhws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "- **Applications**: Credit risk assessment (classify loan approval), medical diagnosis (classify disease based on symptoms), customer segmentation (classify buying behavior).\n",
        "- **Advantages**: Easy to interpret (visual tree), handle non-linear data, no need for scaling.\n",
        "- **Limitations**: Prone to overfitting, sensitive to small changes in data, biased toward features with more levels."
      ],
      "metadata": {
        "id": "sUJDleimOhtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write a Python program to: A. Load the Iris Dataset B. Train a Decision Tree Classifier using the Gini criterion C. Print the model's accuracy and feature importances"
      ],
      "metadata": {
        "id": "vPEquJbkOhpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# A. Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# B. Train Decision Tree Classifier with Gini\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# C. Print accuracy and feature importances\n",
        "print(f\"My Model Accuracy: {accuracy}\")\n",
        "print(f\"My Feature Importances: {clf.feature_importances_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzDEJo_vQtYM",
        "outputId": "0194bb34-65cb-49ce-8bdd-06eedfb5379e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My Model Accuracy: 1.0\n",
            "My Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Write a Python program to: A. Load the Iris Dataset B. Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
      ],
      "metadata": {
        "id": "mkkjb49jOhnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# A. Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# B. Train with max_depth=3\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "y_pred_pruned = clf_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# Train fully-grown tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Compare\n",
        "print(f\"Pruned Tree Accuracy (max_depth=3): {accuracy_pruned}\")\n",
        "print(f\"Full Tree Accuracy: {accuracy_full}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cKojgQnRDis",
        "outputId": "ebfe07be-ad44-420a-d606-ae4c0b2939c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned Tree Accuracy (max_depth=3): 1.0\n",
            "Full Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Write a Python program to: • Load the California Housing dataset from sklearn • Train a Decision Tree Regressor . Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "sCmaPelBOhkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing Dataset\n",
        "california = fetch_california_housing()\n",
        "X = california.data\n",
        "y = california.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and MSE\n",
        "y_pred = reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print MSE and feature importances\n",
        "print(f\"MSE: {mse}\")  # Using your name\n",
        "print(f\"Feature Importances: {reg.feature_importances_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv1keSO7Raly",
        "outputId": "61749ced-5c94-44d7-fac9-a2e4696c1ea4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.495235205629094\n",
            "Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n",
            " 0.09371656 0.08290203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Write a Python program to: • Load the Iris Dataset Tune the Decision Tree's max_depth and min_samples_split using GridSearchCV • Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "QNic2i1xRjnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tune with GridSearchCV\n",
        "param_grid = {'max_depth': [3, 5, None], 'min_samples_split': [2, 5, 10]}\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_params = grid_search.best_params_\n",
        "best_clf = grid_search.best_estimator_\n",
        "y_pred = best_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKT7s36VPWQN",
        "outputId": "7d949be9-c104-4162-a773-9b8647a06a7e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: Imagine you're working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "Answer:-\n",
        "A. Handle the missing values\n",
        "B. Encode the categorical features\n",
        "C. Train a Decision Tree model\n",
        "D. Tune its hyperparameters\n",
        "E. Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "**Step-by-Step Process**:\n",
        "A. **Handle Missing Values**: Identify with `df.isnull().sum()`. For numerical (e.g., blood pressure), impute with mean/median; for categorical (e.g., gender), use mode. Drop rows if missingness >50% or use KNN imputation for complex cases. Reason: Prevents bias and retains data.\n",
        "\n",
        "B. **Encode Categorical Features**: Use one-hot encoding for nominal (e.g., symptoms: 'fever', 'cough' → binary columns) via `pd.get_dummies()`. For ordinal (e.g., severity: 'low', 'high'), use label encoding. Reason: Decision Trees handle encoded data better.\n",
        "\n",
        "C. **Train a Decision Tree Model**: Split data (80/20) using `train_test_split`. Train `DecisionTreeClassifier` with `fit(X_train, y_train)`. Use Gini or entropy criterion.\n",
        "\n",
        "D. **Tune Hyperparameters**: Use `GridSearchCV` to tune `max_depth`, `min_samples_split`, `min_samples_leaf` with CV=5. Fit on train data, get best params.\n",
        "\n",
        "E. **Evaluate Performance**: Use accuracy, F1-score (for imbalance), confusion matrix, and ROC-AUC on test data. Cross-validate for robustness.\n",
        "\n",
        "\n",
        "###**Business Value**:\n",
        "The model enables early disease detection, optimizing resource allocation (e.g., prioritizing high-risk patients), reducing costs (e.g., by 20% through targeted treatments), and improving patient outcomes (e.g., 15% faster recovery), enhancing the company’s reputation and revenue. For XYZ company, it could personalize care for 76-year-old patients."
      ],
      "metadata": {
        "id": "meIigMCANTTS"
      }
    }
  ]
}